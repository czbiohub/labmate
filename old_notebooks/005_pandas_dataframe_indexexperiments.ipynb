{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import catheat\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from functools import reduce \n",
    "\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locate files\n",
    "Define path to the directory that contains your files. Ideally for each experiment you will copy a folder that contains the template for these files and create your dataframe within this folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv\"\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Step 1) Create list of paths in order to read in needed files. \n",
    "This list will contain the full path location to each file in the directory that contains your saved plate map csv's. This is needed so that we can read in each file later. In order to read these cvs's into python the program needs to know where to locate them. If the jupyter notebook is open in the flder where your files exist, you do not need to specify a path, however, if you have your files saved elsewhere this will allow you to access them regardless of where the notebook is opened. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 simple case:\n",
    "To create the path to one file in your directory you simply type out the entire path name. You can save it into a list or as a variable. The name of the path can be easily accessed by entering the folder that contains your files in the terminal and typing the command `pwd` to get the full path to that file. You simply add the file name with it's extension at the end of this path and copy this into your list or save it as a variable name for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/cell_number.csv\n",
      "['/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/cell_number.csv', '/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/experimental_or_control.csv']\n"
     ]
    }
   ],
   "source": [
    "## file path variable\n",
    "file_path_variable='/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/cell_number.csv'\n",
    "\n",
    "## file path list \n",
    "file_path_example_list = ['/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/cell_number.csv','/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/experimental_or_control.csv'] \n",
    "\n",
    "print(file_path_variable)\n",
    "print(file_path_example_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 complex case:\n",
    "If you want to read in many csv's at once you can use the below command. This allows you to read in files in a folder that contain a specific shared characteristsic. In this case the .csv extension. This could also be used to select only files with other shared naming features. For example, if you wanted only files pertaining to cell number ( cell_number.csv and cell_number_1.csv calling the same command with `all_csv=glob.glob(\"{}/*cell_number*.csv\".format(path))` will return a list containing any files that have the path name, followed by any characters, a cell number designation, any other charracters, and then a .csv extension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/cell_number.csv',\n",
       " '/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/cell_number_1.csv',\n",
       " '/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/experimental_or_control.csv',\n",
       " '/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/finalyay.csv',\n",
       " '/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/lonza_nucleofection_buffer.csv']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f\"{path}/filename\" or \"{}/filename\".format(path), \"{my_path}/filename\".format(my_path=path}\n",
    "all_csv=glob.glob(\"{}/*.csv\".format(path))\n",
    "all_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code explained:\n",
    "\n",
    "1) `glob.glob` The glob package allows access files using terminal/Unix calls. In this case, we are using the wild card to collect all the files that end with the extension \".csv\" in the directory specified by path. We imported the matlab library glob to be able to use this function. \n",
    "\n",
    "2) There are several ways to combine the variable `path` which contains the string that references the absolute path to your files with each file that ends in .csv in that directory. The .format method sets an empty pair of curly brackets to be combined with /csv. The argument in .format is the variable that will be passed into those empty brackets. You could also write this as `all_csv=glob.glob(\"{my_path}/*.csv\".format(my_path=path)` or `all_csv=glob.glob(f\"{path}/*.csv\")`. \n",
    "\n",
    "2a) `.format` method explained. This is a method used on strings. In the case above, we created a string with an empty set of brackets for which the path to the csv is needed. Using the .format method on this string allows you to pass an argument into this method that will fill in the value placed in the argument in the location in the string where the empty bracket placeholders are. This is useful here because it allows us to change the `path` vairable in the above code to collect files from any pre-designated path which we save to the variable `path`.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Access file name from their path names. \n",
    "We will want to access the file names from their full path names so that as each dataframe is read into the notebook we can name it according to it's file name and also use this file name to save information identifying information about the files we read in later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a) Simple case for one file.\n",
    "What we want to do here is extract the file name from the path extension and also remove the .csv extenson. \n",
    "\n",
    "This will require the inport of os.path which is a module that allows useful path name manipulations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a.1) first we remove the path name \n",
    "This returns a string that have only the file_name.ext using the .basename() method. \n",
    "This method is the same as returning the second element in the tuple created by calling the .split method on your path name. The .split method splits your path into two components, head and tail, where tail is the very last pathname component and head is everything leading up to this. Notice below that if you call the second element ( index 1) in the tuple, you get only the file_name.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_number.csv\n",
      "('/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv', 'cell_number.csv')\n",
      "cell_number.csv\n"
     ]
    }
   ],
   "source": [
    "print(os.path.basename('/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/cell_number.csv'))\n",
    "print(os.path.split('/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/cell_number.csv'))\n",
    "print(os.path.split('/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/cell_number.csv')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a.2) second we remove the .csv extension\n",
    "This returns a string that have only the file_name using the .splitext() method. \n",
    "This method is similar to the .split method. In this case, it will split an input a tuple containing two components, the path and the extension. We then access the path, which in this case will be first element ( index = 0) in this tuple. \n",
    "\n",
    "If we combine these two functions we can call the `.basename` method to extract the file_name.csv and the .splitext method followed by tuple element selection to access the file_name alone. For one file, we would save this to a variable called `one_file_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cell_number', '.csv')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.splitext('cell_number.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cell_number'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.splitext('cell_number.csv')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cell_number'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_file_name=os.path.splitext(os.path.basename('/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/plate_csv/cell_number.csv'))[0]\n",
    "one_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b) Complex case for many files \n",
    "In this case we want to do this opperation many times over on all the files we created in the `all_csv` variable that we created earlier which contains the full path to each of the csvs we created in our experimental folder. \n",
    "\n",
    "Now we want to create a dictionary that will contain the name of the file and the dataframe associated with the file. \n",
    "\n",
    "We start by initiation an empty dataframe called `read_files`.\n",
    "\n",
    "The we set up a for loop which will loop through each element in our all_csv list and for each element perform a specific opperation. \n",
    "\n",
    "We start by indicating `for file in all_csv` which says that for each iteration of the loop, which will continue for the length of all the elements in all_csv, the element (in this case the string for the full path) will be asigned to the variable file.\n",
    "\n",
    "The variable file, which is set as the first element first, will:\n",
    "\n",
    "first) pass into the `os.path.splitext(os.path.basename(file))[0]` process, which will take the first full path in our list, return only the file name and save it as a variable within the loop called file_name.\n",
    "\n",
    "second) pass into the pd.read_csv() pandas dataframe method which will read in this csv into a data frame and save this dataframe as the varaible df.\n",
    "\n",
    "third) we add this key value pair to the above empty dictionary by specifying the key as the file_name and the value ad the data frame. This ends the loop and the loop will begin again with the next element in the all_csv list. \n",
    "\n",
    "Final result: we have a dictionary that contains the file_name as a key and the corresponding dataframe as the value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Unnamed: 0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2441\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Unnamed: 0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-71f537b7c088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtidy_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m## set and empty dictionary to which we will add key value pairs, your file name and the long format of a given 96 well plate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_frame\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_of_dfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Unnamed: 0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"col_num\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# turn 96 well format into long format, returning a df with a list of values for each well in your 96 well plate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"well_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Unnamed: 0\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"col_num\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# make a new column to combine row letter with col number.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtidy_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mremove_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pass the df into the remove cols function to remove col for row letter and col number and add to dictionary with asoociated file name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36mmelt\u001b[0;34m(frame, id_vars, value_vars, var_name, value_name, col_level)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mmdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mmdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0mmcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_vars\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvar_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalue_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mpop\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdrop\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mRaise\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \"\"\"\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2442\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Unnamed: 0'"
     ]
    }
   ],
   "source": [
    "def remove_cols(df):\n",
    "    ### define a function that will drop unwanted columns from your df along the column axis\n",
    "    cols_to_drop = ['Unnamed: 0', 'col_num'] \n",
    "    removed_cols = df.drop(cols_to_drop, axis=1)\n",
    "    return removed_cols\n",
    "\n",
    "tidy_files={} ## set and empty dictionary to which we will add key value pairs, your file name and the long format of a given 96 well plate\n",
    "for file_name, data_frame in dict_of_dfs.items():\n",
    "    df=pd.melt(data_frame, id_vars=\"Unnamed: 0\", var_name=\"col_num\", value_name=file_name) # turn 96 well format into long format, returning a df with a list of values for each well in your 96 well plate.  \n",
    "    df[\"well_id\"]=df[\"Unnamed: 0\"] + df[\"col_num\"] # make a new column to combine row letter with col number. \n",
    "    tidy_files[file_name]= remove_cols(df) # pass the df into the remove cols function to remove col for row letter and col number and add to dictionary with asoociated file name. \n",
    " \n",
    "dict_of_dfs={}\n",
    "for file in all_csv:\n",
    "        file_name=os.path.splitext(os.path.basename(file))[0] # split based on extension\n",
    "        df=pd.read_csv(file)\n",
    "        dict_of_dfs[file_name]=df\n",
    "print(dict_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tidy_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-468638504d2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtidy_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdfs_merged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"well_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdfs_merged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdfs_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"well_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdfs_merged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tidy_file' is not defined"
     ]
    }
   ],
   "source": [
    "dfs=list(tidy_file.values())\n",
    "dfs_merged=reduce(lambda left,right: pd.merge(left,right, on=\"well_id\"), dfs)\n",
    "dfs_merged=dfs_merged.set_index(\"well_id\")\n",
    "dfs_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change dataframes from 96 well format to long format where A1-H12 is one column and merge all of these dfs together. \n",
    "\n",
    "Here we want to use the pd.melt function to get the variable and value name associated with each cell in the 96 well plates. We do this by calling the pd.melt function on our 96 well plate format and specifying the id vairable ( id_vars) for which we want to get the associated values from each column. In this case we want to know for every value in the column Unnamed: 0, which is out list of row lables(A-H), what is the associated value for each column of the plate. We specify this by identifying the id_vars as the column containing the row letters. We do not specify a value_vars since we want to do this for all of the columns and doing so for all columns id the default. We specify var_name as the name we want to give for the list of alll the column numbers and value_name as the name of the value inside of each well. In this case, we specify value name as `file_name` which is the temporary variable name given to our dictionary keys as it iterates though every key, value pair in our dictionary. Thus, for each iteration, we will name the value column after the file_name from which the dataframe came. Everytime we melt our dataframe, we also want to create a new column called \"well_id\" which will combine the row name and column number for each well. This is done by adding a new column to the dataframe and specifying the values for that column. Finally, since we have the well_id column, we can remove the colum with the rows and cols separateley indicated for each item in the dataframe. We do this by passing the df into the function remove_cols() which we defined above. This function takes in one argument, your df,and removed the list of columns specified in the variable, cols_to_drop along the column axis (axis 1). If we pass our df into the function in the below loop, we will return a df that has the columns dropped, we can then add this df to the empty dictionary tidy_files by specifying the file_name (key from our dict_of_dicts dictionary) as the key for the new df (relating to the value associated with that key (data_frame) which has been transformed into a long format and had the specified columns dropped. \n",
    "\n",
    "Now we want to put all of these columns together. There is not an easy way to do this in python as most functions take only two items and we want to merge many tidy dfs together. For this we will need the reduce function. \n",
    "\n",
    "The reduce function takes in two arguments, one is a function and the other is a list of values over which you want to reduce. The reduce function reduces all the values in that list to one value following what is specified by the function. How this works is that reduce first take out the first element in your list, this is the set up for the reduce function. It then does something with that element and the next element in the list, as specified by the function you select, and returns an item that is of the same type as the elements in your list, which is a new item that has had the function performed on the first two items in the list. Now this new element will be used as the input to the remove function where it will perform the function on this new item and the next item in the list. In this way you iterate through all items in the list combining them based on the function of choice. In the case below what we want to do is merge the dataframes together based on the column \"well_id\". We use lambda, to create an unnamed function within our script that takes in two arguments, left and right, on these arguments, it will perform the merge function, which takes in two dataframes, left and right, and merges them based on the specification in the function call, in this case, on well_id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols(df):\n",
    "    ### define a function that will drop unwanted columns from your df along the column axis\n",
    "    cols_to_drop = ['Unnamed: 0', 'col_num'] \n",
    "    removed_cols = df.drop(cols_to_drop, axis=1)\n",
    "    return removed_cols\n",
    "\n",
    "tidy_files={} ## set and empty dictionary to which we will add key value pairs, your file name and the long format of a given 96 well plate\n",
    "for file_name, data_frame in dict_of_dfs.items():\n",
    "    df=pd.melt(data_frame, id_vars=\"Unnamed: 0\", var_name=\"col_num\", value_name=file_name) # turn 96 well format into long format, returning a df with a list of values for each well in your 96 well plate.  \n",
    "    df[\"well_id\"]=df[\"Unnamed: 0\"] + df[\"col_num\"] # make a new column to combine row letter with col number. \n",
    "    tidy_files[file_name]= remove_cols(df) # pass the df into the remove cols function to remove col for row letter and col number and add to dictionary with asoociated file name. \n",
    " \n",
    "\n",
    "dfs=list(tidy_file.values()) # create list of tidy ataframes\n",
    "dfs_merged=reduce(lambda left,right: pd.merge(left,right, on=\"well_id\"), dfs) # merge all dataframes together on well_id\n",
    "dfs_merged=dfs_merged.set_index(\"well_id\") # set well_id as the index for your new data frame. \n",
    "\n",
    "dfs_merged.to_csv(\"{}/finalyay.csv\".format(path)) ## if finished you can save this as a csv into the folder from which your files came. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord1_384=pd.read_csv('/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/Coord1.csv')\n",
    "coord1_384.set_index(\"well_id_96\")\n",
    "coord1_384.shape\n",
    "coord1_384.head()\n",
    "\n",
    "i7_index=pd.read_csv('/Users/giana.cirolia/Desktop/Nucleofection_Pipelne_Code/i7_index.csv')\n",
    "i7_index.head()\n",
    "i7_index.shape\n",
    "\n",
    "all_i7=list(i7_index[\"I7_index_well_384\"])\n",
    "coord1=list(coord1_384['Cord1_well_id_384'])\n",
    "\n",
    "i7_index_coord1=pd.merge(i7_index,coord1_384, left_on=\"I7_index_well_384\", right_on=\"Cord1_well_id_384\" )\n",
    "i7_index_coord1.set_index('well_id_96') \n",
    "i7_index_coord1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a summary of each of your meta data values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols=dfs_merged.columns # gives a list of the columns from your merged data frame.\n",
    "print(cols)\n",
    "length=len(cols) \n",
    "print(length)\n",
    "\n",
    "all_counts={} # set empty dictionary. \n",
    "for i in range(length): # iterage over the functions below for as many columns as there are in the dataframe. \n",
    "    col_name=(cols[i]) # select the column name you will use each time by passing the range values 0-length of col into the cols variable to produce the col name at that index. \n",
    "\n",
    "## summarize your conditions in each section. \n",
    "    counts={} # set an emty dictionary. \n",
    "    for cond in dfs_merged[col_name]: # cond is the value in a specific column on the df_merged data frame which we specify diffeently on each interation of the loop. \n",
    "        if cond in counts.keys(): # for this condition, if the condition already has a key made for it in the counts dictionary, we add one to the value associate with that key \n",
    "            counts[cond]+=1\n",
    "        else:\n",
    "            counts[cond]=1 # if the above condition evaluates to false, then this is a new key and we add it to the counts dictionary and asign the value 1 to this new key. \n",
    "        all_counts[col_name]=counts # once this has been evaluated for all values in the specific column, then add this value, along with it's key, to the all_counts dictionary. \n",
    "    \n",
    "print(all_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "cols=dfs_merged.columns\n",
    "print(cols)\n",
    "length=len(cols)\n",
    "print(length)\n",
    "\n",
    "all_counts={}\n",
    "for i in range(length):\n",
    "    col_name=(cols[i])\n",
    "\n",
    "## summarize your conditions in each section. \n",
    "    counts={}\n",
    "    for cond in dfs_merged[col_name]:\n",
    "        if cond in counts.keys():\n",
    "            counts[cond]+=1\n",
    "        else:\n",
    "            counts[cond]=1\n",
    "        all_counts[col_name]=counts\n",
    "    \n",
    "print(all_counts)\n",
    "\n",
    "with open(\"summary.csv\", \"wb\") as f: # use with open to write a file. \n",
    "    w = csv.DictWriter(f,all_counts.keys()) # gives an obect where we can tell it to write a line, creates row based on keys\n",
    "    w.writeheader()# knows from DictWriter and writes them in\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[0].experimental_or_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_cols(df)\n",
    "    cols_to_drop = ['row_letter', 'column_num']\n",
    "    reset_index = df.set_index(\"well_id\")\n",
    "    reset_index = reset_index.drop(cols_to_drop, axis=1)\n",
    "    return reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lonza_nucleofection_buffer_96w_renamed_tidy = pd.melt(lonza_nucleofection_buffer_96w_renamed, \n",
    "                            id_vars= \"row_letter\", \n",
    "                            var_name=\"column_num\",\n",
    "                            value_name=\"lonza_nucleofection_buffer\") #restructure data\n",
    "\n",
    "lonza_nucleofection_buffer_96w_renamed_tidy[\"well_id\"] = lonza_nucleofection_buffer_96w_renamed_tidy.row_letter + \\\n",
    "    lonza_nucleofection_buffer_96w_renamed_tidy.column_num\n",
    "print(lonza_nucleofection_buffer_96w_renamed_tidy.shape)\n",
    "lonza_nucleofection_buffer_96w_renamed_tidy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lonza_nucleofection_buffer_96w= pd.read_csv('lonza_nucleofection_buffer.csv') # read in csv \n",
    "print(lonza_nucleofection_buffer_96w.shape)\n",
    "lonza_nucleofection_buffer_96w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catheat.heatmap(lonza_nucleofection_buffer_96w.set_index(\"Unnamed: 0\"), palette='Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import values for experimental or control designation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experimental_or_control_well= pd.read_csv('experimental_or_control.csv') # read in csv \n",
    "print(experimental_or_control_well.shape)\n",
    "experimental_or_control_well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catheat.heatmap(experimental_or_control_well.set_index(\"Unnamed: 0\"), palette='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change column names \n",
    "change unnamed: 0 to test_plate_96w_row_name (plate csv name _ row_names) so that we know that these indexes correspond to this plate. Do so by passing the name of the collumn to be renamed into a dictonary where you give the value of the name to be changed and the desired new name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change name of experimental or control well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_or_control_well_renamed=experimental_or_control_well.rename(columns={\"Unnamed: 0\":\"row_letter\"})\n",
    "print(experimental_or_control_well_renamed.shape)\n",
    "print(experimental_or_control_well_renamed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change name for loza nucleofection buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonza_nucleofection_buffer_96w_renamed=lonza_nucleofection_buffer_96w.rename(columns={\"Unnamed: 0\":\"row_letter\"})\n",
    "print(lonza_nucleofection_buffer_96w_renamed.shape)\n",
    "print(lonza_nucleofection_buffer_96w_renamed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other option that did not work to ask Olga about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructure the dataset based on the row labels of a 96 well plate. \n",
    "For the pd.melt function we take id_vars which is the variable for which we want get the values corresponding to other variables in the table. In this case, we want the values for all other variables, which are all other collumns, because we did not specify a specific subset of variables. If we wanted to specify this subset we could use value_vars and pass in a list of the other collumns (variables) that we want to see the values for relative to column 0. For example, if we only selected collumn 1, we would see the values at each position in the id_vars collumn from the corresponding index position in column 1. We can name these columns for clarity. \n",
    "\n",
    "### Print out information about new data frame\n",
    "• `.shape` to get the dimensions of the dataframe. Here we have 96 rows and 3 collumns. This is because we are getting values from every collumn in the plate (12) for each of the 8 row labels (A-H) in collumn unnamed: 0. \n",
    "\n",
    "• `.head()` to view the first entries in the dataframe. This requires parenthasis in calling the attribute as .head is a function that can take in arguments and is not intrinsic to the data frame, in that you canspecify how many rows you want to see. \n",
    "\n",
    "• `.index or .index.values` to get the index range (0-96) which means it godes from 0 up to 95 not including 95 in steps of 1. or .index.values to get the actual values of the index, a list from 0-95. \n",
    "\n",
    "• `.columns` to get the collumn names. \n",
    "\n",
    "• `type()` with the dataframe as the argument to get the type of the object. In this case we check that the object is still a dataframe. This is important because there are certain ways of selecting from a dataframe where the sliced object, if only one column can become a series not a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tidy format of nucleofection buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonza_nucleofection_buffer_96w_renamed_tidy = pd.melt(lonza_nucleofection_buffer_96w_renamed, \n",
    "                            id_vars= \"row_letter\", \n",
    "                            var_name=\"column_num\",\n",
    "                            value_name=\"lonza_nucleofection_buffer\") #restructure data\n",
    "\n",
    "lonza_nucleofection_buffer_96w_renamed_tidy[\"well_id\"] = lonza_nucleofection_buffer_96w_renamed_tidy.row_letter + \\\n",
    "    lonza_nucleofection_buffer_96w_renamed_tidy.column_num\n",
    "print(lonza_nucleofection_buffer_96w_renamed_tidy.shape)\n",
    "lonza_nucleofection_buffer_96w_renamed_tidy.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tidy format for experimental condition plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_or_control_well_renamed_tidy = pd.melt(experimental_or_control_well_renamed, \n",
    "                            id_vars= \"row_letter\", \n",
    "                            var_name=\"column_num\",\n",
    "                            value_name=\"experimental_or_control\") #restructure data\n",
    "experimental_or_control_well_renamed_tidy[\"well_id\"] = experimental_or_control_well_renamed_tidy.row_letter + \\\n",
    "    experimental_or_control_well_renamed_tidy.column_num\n",
    "print(experimental_or_control_well_renamed_tidy.shape)\n",
    "experimental_or_control_well_renamed_tidy.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Merge tidy data frames \n",
    "notice that inner, outer, right and left did the same thing why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = lonza_nucleofection_buffer_96w_renamed_tidy.merge(experimental_or_control_well_renamed_tidy, left_on=\"well_id\" , right_on=\"well_id\")\n",
    "print(merged_data.shape)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_renamed=merged_data.rename(columns={\"row_letter_x\":\"96_well_row_letter\"}) # how to change col labels. \n",
    "print(merged_data_renamed.shape)\n",
    "merged_data_renamed.head\n",
    "merged_data_renamed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_or_control_well_renamed_tidy.head() # don't want row letter and col number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "experimental_or_control_well_renamed_tidy_shuffled = pd.DataFrame(experimental_or_control_well_renamed_tidy, \n",
    "                                                                  index=np.random.permutation(experimental_or_control_well_renamed_tidy.index))\n",
    "experimental_or_control_well_renamed_tidy_shuffled.head()\n",
    "indextest=np.random.permutation(experimental_or_control_well_renamed_tidy.index)\n",
    "print(indextest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = lonza_nucleofection_buffer_96w_renamed_tidy.merge(experimental_or_control_well_renamed_tidy_shuffled, \n",
    "                                                                left_on=\"well_id\" , right_on=\"well_id\")\n",
    "print(merged_data.shape)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data=lonza_nucleofection_buffer_96w_tidy.merge(experimental_or_control_well_tidy, left_on=[\"Unnamed: 0\", \"column_num\"], right_on=[\"Unnamed: 0\", \"column_num\"])\n",
    "print(merged_data.shape)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge on just one column\n",
    "merge on row labels: this lists all the aligns the data so that the row labels are shared. For the left data frame the column labels are repeated and aligned with the unique column labled, from 1-12 for the right data frame. \n",
    "\n",
    "merge on column labels: this gives the first column for both of the data sets shared. For the left df it repeats that row lables (A-H) and gives a unqiue value A-H for the right data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data=lonza_nucleofection_buffer_96w_tidy.merge(experimental_or_control_well_tidy, left_on=[\"Unnamed: 0\"], right_on=[\"Unnamed: 0\"])\n",
    "print(merged_data.shape)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data=lonza_nucleofection_buffer_96w_tidy.merge(experimental_or_control_well_tidy, left_on=[\"column_num\"], right_on=[\"column_num\"])\n",
    "print(merged_data.shape)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Joining well_type and buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['row_letter', 'column_num']\n",
    "\n",
    "buffers = lonza_nucleofection_buffer_96w_renamed_tidy.set_index(\"well_id\")\n",
    "buffers = buffers.drop(cols_to_drop, axis=1)\n",
    "buffers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "well_type = experimental_or_control_well_renamed_tidy.set_index(\"well_id\")\n",
    "well_type = well_type.drop(cols_to_drop, axis=1)\n",
    "well_type.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined=well_type.join(buffers)\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarize your conditions in each section. \n",
    "\n",
    "counts={}\n",
    "joined[\"experimental_or_control\"]\n",
    "for cond in joined[\"experimental_or_control\"]:\n",
    "    if cond in counts.keys():\n",
    "        counts[cond]+=1\n",
    "    else:\n",
    "        counts[cond]=1\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=joined.columns\n",
    "cols\n",
    "y=cols[0]\n",
    "y\n",
    "\n",
    "all_counts={}\n",
    "\n",
    "## summarize your conditions in each section. \n",
    "cols=joined.columns\n",
    "counts={}\n",
    "for cond in joined[y]:\n",
    "    if cond in counts.keys():\n",
    "        counts[cond]+=1\n",
    "    else:\n",
    "        counts[cond]=1\n",
    "    all_counts[y]=counts\n",
    "    \n",
    "print(all_counts)\n",
    "len(all_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cols)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##loop through all columns and get summary information \n",
    "\n",
    "cols=joined.columns\n",
    "print(cols)\n",
    "length=len(cols)\n",
    "print(length)\n",
    "print(range(1))\n",
    "\n",
    "all_counts={}\n",
    "for i in range(length):\n",
    "    col_name=(cols[i])\n",
    "\n",
    "## summarize your conditions in each section. \n",
    "    counts={}\n",
    "    for cond in joined[col_name]:\n",
    "        if cond in counts.keys():\n",
    "            counts[cond]+=1\n",
    "        else:\n",
    "            counts[cond]=1\n",
    "        all_counts[col_name]=counts\n",
    "    \n",
    "print(all_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_type_scrambled = well_type.loc[scrambled_wells] # what are we doing here. \n",
    "well_type_scrambled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_type_scrambled.join(buffers)\n",
    "buffers.join(well_type_scrambled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wells_tidy['column_number'] = wells_tidy['Empty96ColNum'].astype(int)\n",
    "print(wells_tidy.shape)\n",
    "wells_tidy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong way! We did numbers, lettesr for `wells_tidy` and letters, numbers for `buffer_tidy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wells_tidy.merge(buffer_tidy, left_on=[ \"Empty96ColNum\", 'Unnamed: 0',], \n",
    "                 right_on=['row_letter', 'column_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wells_tidy.merge(buffer_tidy, left_on=['Unnamed: 0', \"Empty96ColNum\"], \n",
    "                 right_on=['row_letter', 'column_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wells_tidy.merge(buffer_tidy, left_on=['Unnamed: 0', \"column_number\"], \n",
    "                 right_on=['row_letter', 'column_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Empty96W_ColumnList.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(Empty96W_ColumnList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SelectValues_Empty96WellID=Empty96W_ColumnList.iloc[:,1] # select needed collumn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SelectValues_Empty96WellID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SelectValues_Empty96WellID_df=SelectValues_Empty96WellID.to_frame() # turn series into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SelectValues_Empty96WellID_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(SelectValues_Empty96WellID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Empty96W_ColumnList2=pd.melt(empty_96W, var_name=\"Empty96ColNum2\",value_name=\"Empty96WellID2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SelectValues_Empty96WellID2=Empty96W_ColumnList2.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SelectValues_Empty96WellID2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SelectValues_Empty96WellID2_df=SelectValues_Empty96WellID2.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SelectValues_Empty96WellID2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([SelectValues_Empty96WellID_df,SelectValues_Empty96WellID2_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
